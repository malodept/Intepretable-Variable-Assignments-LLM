# -*- coding: utf-8 -*-
"""fine-tuning_cassiop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pmj-4mWvQMQhGp74kZCV1NAD1Wg9EGcj
"""

!pip install bitsandbytes
!pip install transformers accelerate peft datasets

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
from datasets import Dataset
import torch
from huggingface_hub import login
from datetime import datetime
from datasets import load_dataset


# Authentification Hugging Face

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# Configuration quantization 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    llm_int8_enable_fp32_cpu_offload=True
)

# Load tokenizer & model (quantized + LoRA-compatible)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
model = prepare_model_for_kbit_training(model)

# Dataset JSONL
dataset = Dataset.from_json("/content/sample_data/dataset_by_hand_l.jsonl")

# Tokenization
def tokenize(example):
    full_prompt = f"User: {example['input']}\nAssistant: "
    example["input_ids"] = tokenizer(full_prompt, truncation=True, padding="max_length", max_length=512)["input_ids"]
    example["labels"] = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512)["input_ids"]
    return example

tokenized_dataset = dataset.map(tokenize)

# LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# Entra√Ænement
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    output_dir="./mistral-lora-finetuned",
    save_strategy="epoch",
    save_total_limit=1,
    bf16=False,
    fp16=True,
    logging_dir=f"./logs-{datetime.now().strftime('%H-%M-%S')}",
    logging_first_step=True,
    report_to="none",
    disable_tqdm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

# Train!
trainer.train()

# Save final model and tokenizer
model.save_pretrained("/kaggle/working/mistral-lora-finetuned")
tokenizer.save_pretrained("/kaggle/working/mistral-lora-finetuned")